{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pacman environment and how the game works - \n",
    "\n",
    "In Pacman, the player earns points by eating pellets and avoiding monsters (contact with one causes Ms.PacMan to lose a life)[src: https://en.wikipedia.org/wiki/Ms._Pac-Man ].\n",
    "\n",
    "The game has four different mazes that appear in different color schemes, and alternate after each of the game's intermissions are seen. The pink maze appears in levels 1 & 2, the light blue maze appears in levels 3, 4, & 5, the brown maze appears in levels 6 through 9, and the dark blue maze appears in levels 10 through 14. After level 14, the maze configurations alternate every 4th level.\n",
    "Three of the four mazes (the first, second, and fourth ones) have two sets of warp tunnels, as opposed to only one in the original maze.\n",
    "\n",
    "The walls have a solid color rather than an outline, which makes it easier for a novice player to see where the paths around the mazes are.\n",
    "\n",
    "#### Possible movements = 9, left, right, up, down, centre, upper-left, upper-right, lower-left, lower-right. \n",
    "\n",
    "\n",
    "## Using DQN for training Pacman in Open AI Gym - \n",
    "\n",
    "There are 250 pellets that can be eaten by Ms Pacman. Neural Networks are very good at learning large number of features for highly structured data. Hence, we can use the idea of Q-learning where the Q-function will be represented by a neural network with states and actions as the inputs and returns Q-values as the outputs. \n",
    "\n",
    "To make the agent learn to play the game on its own, we feed it to a Deep Learning model, thereby using DQN to estimate best Q-values for the agent in its environment. Thus, in the DQN, we have 4 quantities - state (s), action (a), reward (r), next state (s'). So, our Q-table would be generated using the following idea - \n",
    "\n",
    "* Use forward propagation to predict Q-values for current state s and all actions a.\n",
    "\n",
    "* Generate the maximum overall networks $Q_{max}(s,a)$ for the next state s.\n",
    "\n",
    "* Generate Q-values for target action using the following formula - \n",
    "\n",
    "$$ y(s,a) = r + \\gamma.max_{a'} Q_{target}(s', a')$$\n",
    "\n",
    "\n",
    "### Preprocessing the Pacman environment for feeding to the Neural Network\n",
    "\n",
    "Each state for the Pacman environment is basically a frame of the game screen which is stored as a numpy array - 3D as an RGB image of the screen. \n",
    "\n",
    "Feeding a 3D array to the NN is difficult to train and highly computationally expensive. Hence, we do the basic preprocessing as follows - \n",
    "\n",
    "* Scale down the image / frame down to 88 x 80 grid size.\n",
    "* Next, we convert the RGB image to the greyscale and \n",
    "* Generate the contrasted image for sharper pixels\n",
    "\n",
    "We develop our Deep Learning Model as a Convolutional Neural Network with the following specifications - \n",
    "\n",
    "* 3 convolutional layers\n",
    "    * 1st layer - 32 nodes, 8x8 convolution mask, strides = 4, downsized to shape = 22x20x32, relu\n",
    "    * 2nd layer - 64 nodes, 4x4 convolution mask, strides = 2, downsized to shape = 11x10x64, relu\n",
    "    * 3rd layer - 64 nodes, 3x3 convolution mask, strides = 1, downsized to shape = 11x10x64, relu\n",
    "    \n",
    "* 2 fully connected layers\n",
    "    * 1st layer = 512 nodes, relu, shape = 11x10x64\n",
    "    * 2nd and final layer = 9 nodes for 9 different possible actions\n",
    "    \n",
    "Apart from the normal agent's DQN, we also need to maintain another Deep learning model for the target. So, we define a DQN model for the target as well, which estimates the next state's Q-values for each possible action to compute the target Q-values for training the agent DQN. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pacman_Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define environment parameters\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=5000)\n",
    "        \n",
    "        # Define Hyperparameters to learn through deep learning\n",
    "        self.gamma = 0.95            # discount rate\n",
    "        self.epsilon = 1.0          # exploration rate to start\n",
    "        self.epsilon_min = 0.1      # minimum exploration rate (epsilon-greedy)\n",
    "        self.epsilon_decay = 0.995  # decay rate for epsilon\n",
    "        self.learning_rate = 0.001\n",
    "        self.update_rate = 1000     # steps needed until target network gets updated\n",
    "        \n",
    "        # Construct DQN models\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.model.summary()\n",
    "\n",
    "    # Build the CNN for DQN on the pacman environment \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Defining the Conv Layers - EXTRACTING FEATURES FROM ENVIRONMENT\n",
    "        # 1st layer - output size = 32 nodes, kernel size = 8x8, skipping steps (strides) = 4 pixels,\n",
    "        # input_size = initial state size of the environment\n",
    "        model.add(Conv2D(32, (8, 8), strides=4, padding='same', input_shape=self.state_size))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        #2nd layer - output size = 64 nodes, kernel size = 4x4, strides reduced to 2 pixels\n",
    "        model.add(Conv2D(64, (4, 4), strides=2, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        #3rd layer - output size = 64 nodes, kernel size = 3x3, strides reduced to 1 pixel\n",
    "        model.add(Conv2D(64, (3, 3), strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "\n",
    "        # Defining the Fully Connected Neural Nets to classify features\n",
    "        # 1st layer - input size = 512 nodes, 2nd layer - output size = number of possible actions of the agent\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "        #generate model\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    #Function to save past experiences as replays\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # Function to select actions based on epsilon-greedy method\n",
    "    def act(self, state):\n",
    "        # Explore randomly\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        #Generate probability of using each random action\n",
    "        actions = self.model.predict(state)\n",
    "        return np.argmax(actions[0])  # Returns action using policy\n",
    "\n",
    "    # Randomly select some actions and train the agent from previos replay experiences \n",
    "    def replay(self, batch_size):\n",
    "        # perform replay on a small batch of the stored experiences \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # checking if the agent has reached target\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.target_model.predict(next_state)))\n",
    "            else:\n",
    "                target = reward\n",
    "                \n",
    "            # Making new targets\n",
    "            # Output the Q-value predictions\n",
    "            target_f = self.model.predict(state)\n",
    "            \n",
    "            # Update the chosen action value with the new computed target\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            # Train the new model based on the new target and current state\n",
    "            # for next possible action\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        \n",
    "        # decay the epsilon every time it grows larger than the minimum epsilon \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    #Update the target model based on the current computed values\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "            \n",
    "    #load a saved model\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    #save parameters of trained models\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that helps in joining 4 images together after training\n",
    "# and returns an average of the combined images\n",
    "def join_images(images, state_size = (88, 80, 1)):\n",
    "    new_dim_Arr = np.zeros((state_size), np.float64)\n",
    "    avg_image = np.expand_dims(new_dim_Arr, axis=0)\n",
    "\n",
    "    for img in images:\n",
    "        avg_image += img\n",
    "        \n",
    "    if len(images) < 4:\n",
    "        return avg_image / len(images)\n",
    "    else:\n",
    "        return avg_image / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for preprocessing each frame, src: github.com/ageron/tiny-dqn\n",
    "def process_frame(frame, state_size = (88, 80, 1)):\n",
    "    pacman = np.array([210, 164, 74]).mean()\n",
    "    img = frame[1:176:2, ::2]    # Crop and downsize\n",
    "    img = img.mean(axis=2)       # Convert to greyscale\n",
    "    img[ img==pacman ] = 0 # Improve contrast by making pacman white\n",
    "    # Normalize each image between -1 and 1.\n",
    "    img = img/128\n",
    "    new_dim_img = img.reshape(state_size)\n",
    "    \n",
    "    return np.expand_dims(new_dim_img, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's define the environment and associated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 22, 20, 32)        2080      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 22, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 10, 64)        32832     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 11, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 10, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 11, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 7040)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3604992   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 4617      \n",
      "=================================================================\n",
      "Total params: 3,681,449\n",
      "Trainable params: 3,681,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MsPacman-v0')\n",
    "state_size = (88, 80, 1)\n",
    "action_size = env.action_space.n\n",
    "agent = Pacman_Agent(state_size, action_size)\n",
    "\n",
    "EPISODES = 100\n",
    "batch_size = 8 #we use 8 elements in a batch at a time for Pacman\n",
    "\n",
    "# wait for 90 actions before an episode begins\n",
    "skips = 90  \n",
    "\n",
    "total_time = 0   # keeping track of total number of steps taken\n",
    "all_rewards = 0  # Used to compute avg reward over time\n",
    "\n",
    "#initial done report = False\n",
    "done = False\n",
    "\n",
    "rewards_over_episodes= []\n",
    "eps_over_episodes= []\n",
    "score_over_episodes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run episodes\n",
    "\n",
    "Start training the agent using DQN and preprocessing of each frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(EPISODES):\n",
    "    total_reward = 0\n",
    "    score= 0\n",
    "    \n",
    "    # Reset environment for every episode, preprocess it and blend 4 images together in every episode\n",
    "    state = process_frame(env.reset())\n",
    "    images = deque(maxlen=4)  \n",
    "    images.append(state)\n",
    "    \n",
    "    # skip actions before start of each episode\n",
    "    for _ in range(skips): \n",
    "        env.step(0)\n",
    "    \n",
    "    # run each episode for a certain time limit\n",
    "    for time in range(1000):\n",
    "        env.render()\n",
    "        total_time += 1\n",
    "        \n",
    "        # Every update_rate timesteps we update the target network parameters\n",
    "        if total_time % agent.update_rate == 0:\n",
    "            agent.update_target_model()\n",
    "        \n",
    "        # Return the avg of the last 4 frames\n",
    "        state = join_images(images)\n",
    "        \n",
    "        # Perform interaction of the agent with the environment \n",
    "        # to get the dynamics and steps to take\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Return the avg of the last 4 frames after processing and\n",
    "        # combining the images\n",
    "        next_state = process_frame(next_state)\n",
    "        images.append(next_state)\n",
    "        next_state = join_images(images)\n",
    "        \n",
    "        # Store replay memory\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        #Update the state\n",
    "        state = next_state\n",
    "        \n",
    "        # Add the reward\n",
    "        score+= reward\n",
    "        \n",
    "        reward -= 1  # Discount to avoid collection of rewards\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            all_rewards += score\n",
    "            \n",
    "            print(\"episode: {}/{}, game score: {}, reward: {}, avg reward: {}, time: {}, total time: {}\"\n",
    "                  .format(ep+1, EPISODES, score, total_reward, all_rewards/(ep+1), time, total_time))\n",
    "            \n",
    "            break\n",
    "        \n",
    "        # if agent learns more than the batch size, send it the stored\n",
    "        # replay experiences of that batch size\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "        \n",
    "    # save epsilon and reward values for each episode\n",
    "    eps_over_episodes.append(agent.epsilon)\n",
    "    rewards_over_episodes.append(total_reward)\n",
    "    score_over_episodes.append(all_rewards)\n",
    "    \n",
    "    # Plot curve of reward vs episode after every 100 episodeps_over_episodes\n",
    "    if (ep+1) % 20 == 0:\n",
    "        plt.figure()\n",
    "        plt.title('Curve of reward vs episodes after completion of ' + str(ep+1) + ' episodeps_over_episodes')\n",
    "        plt.plot(rewards_over_episodes)\n",
    "        plt.show()\n",
    "\n",
    "# plot epsilon and reward curves at the end of the episodes        \n",
    "plt.figure()\n",
    "plt.title('Final curve of reward vs episodes')\n",
    "plt.plot(score_over_episodes)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Final curve of score vs episodes')\n",
    "plt.plot(rewards_over_episodes)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Epsilon-Decay curve')\n",
    "plt.plot(eps_over_episodes)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
